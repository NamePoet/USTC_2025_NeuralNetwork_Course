# å¯¼å…¥å¿…è¦çš„åº“
import os  # æ“ä½œç³»ç»Ÿæ¥å£
import math  # æ•°å­¦å‡½æ•°åº“ï¼Œç”¨äºè®¡ç®— epsilon è¡°å‡
import torch  # PyTorch ä¸»åº“
import torch.nn.functional as F  # æä¾›æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ç­‰
import numpy as np  # æ•°å€¼è®¡ç®—åº“
import matplotlib.pyplot as plt  # å¯è§†åŒ–åº“
import gymnasium as gym
import random  # éšæœºå‡½æ•°åº“
from collections import deque, namedtuple  # åŒç«¯é˜Ÿåˆ—ä¸å‘½åå…ƒç»„
from torch.utils.tensorboard import SummaryWriter  # ç”¨äºæ—¥å¿—è®°å½•å’Œå¯è§†åŒ–
from moviepy.editor import ImageSequenceClip  # å°†å›¾åƒåºåˆ—è½¬æ¢ä¸ºè§†é¢‘ï¼ˆç”¨äºç»“æœå¯è§†åŒ–ï¼‰

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# å®šä¹‰ä¸€ä¸ª Transition å‘½åå…ƒç»„ï¼Œç”¨äºç»éªŒå›æ”¾ä¸­çš„å•ä¸ªæ ·æœ¬
Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])

# å®šä¹‰ç»éªŒå›æ”¾ç¼“å†²åŒºç±»
class ReplayBuffer:
    def __init__(self, capacity):
        # åˆå§‹åŒ–ä¸€ä¸ªæœ€å¤§é•¿åº¦ä¸º capacity çš„åŒç«¯é˜Ÿåˆ—ï¼Œç”¨äºå­˜å‚¨ç»éªŒ
        self.buffer = deque(maxlen=capacity)

    def add(self, *args):
        # æ·»åŠ ä¸€ä¸ª transition åˆ° buffer ä¸­
        self.buffer.append(Transition(*args))

    def sample(self, batch_size):
        # ä»ç»éªŒæ± ä¸­éšæœºé‡‡æ · batch_size ä¸ª transitionï¼Œè¿”å›è§£å‹åçš„ batch
        transitions = random.sample(self.buffer, batch_size)
        return Transition(*zip(*transitions))  # è§£å‹ä¸º (states, actions, rewards, next_states, dones)

    def __len__(self):
        # è¿”å›å½“å‰ buffer ä¸­çš„ç»éªŒæ•°é‡
        return len(self.buffer)

# å®šä¹‰ DQN çš„ç¥ç»ç½‘ç»œç»“æ„
class DeepQNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super().__init__()
        # æ ‡å‡†DQNç½‘ç»œç»“æ„ - ä¸‰å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œ
        self.net = torch.nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šè¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºéšè—ç‰¹å¾
            torch.nn.Linear(state_dim, hidden_dim),
            torch.nn.ReLU(),

            # ç¬¬äºŒå±‚ï¼šæ›´æ·±å±‚æ¬¡çš„éšè—å±‚
            torch.nn.Linear(hidden_dim, hidden_dim),
            torch.nn.ReLU(),

            # è¾“å‡ºå±‚ï¼šç›´æ¥è¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„Qå€¼
            torch.nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, x):
        # ç›´æ¥é€šè¿‡ç½‘ç»œè¾“å‡ºæ‰€æœ‰åŠ¨ä½œçš„Qå€¼
        return self.net(x)

# å®šä¹‰ DQN æ™ºèƒ½ä½“ç±»
class DQNAgent:
    def __init__(self, state_dim, action_dim, hidden_dim=128, gamma=0.99, lr=1e-3,
                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=500,
                 target_update=100, buffer_size=10000, batch_size=64,
                 device='cuda'):

        self.device = torch.device(device)  # æŒ‡å®šè¿è¡Œè®¾å¤‡ï¼ˆCPU/GPUï¼‰
        self.action_dim = action_dim  # åŠ¨ä½œç©ºé—´ç»´åº¦

        # åˆå§‹åŒ–ç­–ç•¥ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.policy_net = DeepQNet(state_dim, hidden_dim, action_dim).to(self.device)
        self.target_net = DeepQNet(state_dim, hidden_dim, action_dim).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())  # åŒæ­¥ç›®æ ‡ç½‘ç»œå‚æ•°

        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr)  # ä¼˜åŒ–å™¨

        self.replay_buffer = ReplayBuffer(buffer_size)  # ç»éªŒå›æ”¾æ± 
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.batch_size = batch_size  # æ‰¹æ¬¡å¤§å°
        self.target_update = target_update  # ç›®æ ‡ç½‘ç»œæ›´æ–°å‘¨æœŸ

        # Îµ-greedy ç­–ç•¥å‚æ•°
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.total_steps = 0  # ç”¨äºè®°å½•æ­¥æ•°ä»¥è¡°å‡ Îµ

        self.writer = SummaryWriter()  # TensorBoard å†™å…¥å™¨

    def take_action(self, state):
        # æ ¹æ® Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \
                  math.exp(-1. * self.total_steps / self.epsilon_decay)
        self.writer.add_scalar("Epsilon", epsilon, self.total_steps)  # è®°å½• Îµ çš„å˜åŒ–
        self.total_steps += 1
        if np.random.rand() < epsilon:
            return np.random.randint(self.action_dim)  # éšæœºæ¢ç´¢
        state = torch.tensor([state], dtype=torch.float32).to(self.device)
        with torch.no_grad():
            return self.policy_net(state).argmax().item()  # è´ªå©ªé€‰æ‹©

    def update(self):
        if len(self.replay_buffer) < self.batch_size:
            return  # æ ·æœ¬ä¸è¶³ï¼Œæš‚ä¸æ›´æ–°

        # ä»ç»éªŒæ± é‡‡æ ·ä¸€æ‰¹æ•°æ®
        transitions = self.replay_buffer.sample(self.batch_size)
        states = torch.tensor(transitions.state, dtype=torch.float32).to(self.device)
        actions = torch.tensor(transitions.action, dtype=torch.int64).unsqueeze(1).to(self.device)
        rewards = torch.tensor(transitions.reward, dtype=torch.float32).unsqueeze(1).to(self.device)
        next_states = torch.tensor(transitions.next_state, dtype=torch.float32).to(self.device)
        dones = torch.tensor(transitions.done, dtype=torch.float32).unsqueeze(1).to(self.device)

        # è®¡ç®—å½“å‰ Q(s,a)
        q_values = self.policy_net(states).gather(1, actions)  # æ ¹æ®åŠ¨ä½œç´¢å¼•å–å¯¹åº”çš„ Q å€¼

        with torch.no_grad():
            next_q_values = self.target_net(next_states).max(1, keepdim=True)[0]

        # TD ç›®æ ‡å€¼è®¡ç®—ï¼šr + Î³ * Q'
        targets = rewards + self.gamma * next_q_values * (1 - dones)

        # è®¡ç®—å‡æ–¹è¯¯å·®æŸå¤±
        loss = F.mse_loss(q_values, targets)

        # åå‘ä¼ æ’­æ›´æ–°ç½‘ç»œ
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # è®°å½•æŸå¤±åˆ° TensorBoard
        self.writer.add_scalar("Loss", loss.item(), self.total_steps)

        # æ¯éš” target_update æ­¥æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.total_steps % self.target_update == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())

    # ä¿å­˜æ¨¡å‹
    def save(self, filename):
        # å°†å½“å‰ç­–ç•¥ç½‘ç»œï¼ˆpolicy_netï¼‰çš„å‚æ•°ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶ä¸­
        torch.save(self.policy_net.state_dict(), filename)

    # åŠ è½½æ¨¡å‹
    def load(self, filename):
        # åŠ è½½ä¿å­˜çš„ç­–ç•¥ç½‘ç»œå‚æ•°åˆ°å½“å‰ç½‘ç»œ
        self.policy_net.load_state_dict(torch.load(filename))
        # åŒæ—¶å°†ç›®æ ‡ç½‘ç»œï¼ˆtarget_netï¼‰å‚æ•°è®¾ç½®ä¸ºç­–ç•¥ç½‘ç»œçš„å‚æ•°
        self.target_net.load_state_dict(self.policy_net.state_dict())

# æµ‹è¯•è®­ç»ƒå¾—åˆ°çš„ç­–ç•¥
def test_policy(env, agent, episodes=5):
    total_returns = []  # ç”¨äºå­˜å‚¨æ¯ä¸ªæµ‹è¯•å›åˆçš„æ€»å›æŠ¥
    for _ in range(episodes):
        state, _ = env.reset()  # é‡ç½®ç¯å¢ƒï¼Œè·å–åˆå§‹çŠ¶æ€
        terminated, truncated = False, False
        total_reward = 0  # åˆå§‹åŒ–æ€»å›æŠ¥
        while not (terminated or truncated):
            action = agent.take_action(state)  # ç”±æ™ºèƒ½ä½“é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)  # æ‰§è¡Œè¯¥åŠ¨ä½œï¼Œè·å–ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±å’Œæ˜¯å¦ç»“æŸ
            state = next_state
            total_reward += reward  # ç´¯åŠ å¥–åŠ±
        total_returns.append(total_reward)  # å°†æœ¬å›åˆçš„æ€»å›æŠ¥åŠ å…¥åˆ—è¡¨
    # è¿”å›æ‰€æœ‰å›åˆçš„å¹³å‡å›æŠ¥
    return np.mean(total_returns)

# å½•åˆ¶è®­ç»ƒè¿‡ç¨‹çš„è§†é¢‘
def record_video(env, agent, filename="mountaincar_demo.mp4", episodes=3, fps=30):
    frames = []  # å­˜å‚¨æ¯ä¸€å¸§å›¾åƒ
    for _ in range(episodes):
        state, _ = env.reset()   # é‡ç½®ç¯å¢ƒï¼Œè·å–åˆå§‹çŠ¶æ€
        terminated, truncated = False, False
        while not (terminated or truncated):
            frame = env.render()  # è·å–å½“å‰ç¯å¢ƒçš„å›¾åƒå¸§
            frames.append(frame)  # å°†å½“å‰å¸§åŠ å…¥åˆ°å¸§åˆ—è¡¨ä¸­
            action = agent.take_action(state)  # ç”±æ™ºèƒ½ä½“é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
            state, _, terminated, truncated, _ = env.step(action)  # æ‰§è¡ŒåŠ¨ä½œå¹¶è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œç»“æŸæ ‡å¿—
    # ä½¿ç”¨ MoviePy å°†æ‰€æœ‰å¸§åˆæˆè§†é¢‘
    clip = ImageSequenceClip(frames, fps=fps)
    clip.write_videofile(filename)  # ä¿å­˜ä¸ºè§†é¢‘æ–‡ä»¶

# ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„å›æŠ¥å›¾
def plot_metrics(returns, moving_avgs):
    plt.figure(figsize=(10, 5))
    plt.plot(returns, label='Return')  # ç»˜åˆ¶æ¯ä¸ªå›åˆçš„å›æŠ¥
    plt.plot(moving_avgs, label='Moving Average (10)')  # ç»˜åˆ¶10ä¸ªå›åˆçš„æ»‘åŠ¨å¹³å‡å›æŠ¥
    # æ·»åŠ æˆåŠŸé˜ˆå€¼çº¿
    success_threshold = -110  # MountainCaræˆåŠŸæ ‡å‡†
    plt.axhline(y=success_threshold, color='r', linestyle='--', label='Success Threshold')
    plt.xlabel('Episode')  # xè½´æ ‡ç­¾ï¼šå›åˆæ•°
    plt.ylabel('Return')  # yè½´æ ‡ç­¾ï¼šå›æŠ¥
    plt.title('DQN Training Return')  # å›¾è¡¨æ ‡é¢˜
    plt.legend()  # æ˜¾ç¤ºå›¾ä¾‹
    plt.grid(True)  # æ˜¾ç¤ºç½‘æ ¼
    plt.show()  # å±•ç¤ºå›¾è¡¨

# è®¡ç®—è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§æŒ‡æ ‡
def compute_stability(rewards, window=10):
    # è®¡ç®—è¿‡å»10ä¸ªå›åˆå¥–åŠ±çš„æ ‡å‡†å·®ï¼Œä½œä¸ºç¨³å®šæ€§æŒ‡æ ‡
    stability = [np.std(rewards[max(0, i - window):i + 1]) for i in range(len(rewards))]
    plt.figure()
    plt.plot(stability, label='Reward Std Deviation')  # ç»˜åˆ¶å¥–åŠ±æ ‡å‡†å·®
    plt.xlabel('Episode')  # xè½´æ ‡ç­¾ï¼šå›åˆæ•°
    plt.ylabel('Std Dev')  # yè½´æ ‡ç­¾ï¼šæ ‡å‡†å·®
    plt.title('Stability Metric')  # å›¾è¡¨æ ‡é¢˜
    plt.grid(True)  # æ˜¾ç¤ºç½‘æ ¼
    plt.legend()  # æ˜¾ç¤ºå›¾ä¾‹
    plt.show()  # å±•ç¤ºå›¾è¡¨

# ä¸»å‡½æ•°
def main():
    # åˆ›å»º MountainCar-v0 ç¯å¢ƒ
    env = gym.make('MountainCar-v0', render_mode='rgb_array')
    seed = 42  # è®¾ç½®éšæœºç§å­
    torch.manual_seed(seed)  # è®¾ç½® PyTorch éšæœºç§å­
    np.random.seed(seed)  # è®¾ç½® Numpy éšæœºç§å­
    random.seed(seed)  # è®¾ç½® Python éšæœºç§å­
    state, _ = env.reset(seed=seed)  # è®¾ç½®ç¯å¢ƒçš„éšæœºç§å­  # è®¾ç½®ç¯å¢ƒçš„éšæœºç§å­

    # è·å–çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´çš„ç»´åº¦
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    # åˆ›å»º DQN æ™ºèƒ½ä½“ï¼Œä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ CPU
    agent = DQNAgent(state_dim, action_dim, device='cuda' if torch.cuda.is_available() else 'cpu')

    num_episodes = 300  # è®¾ç½®æ€»çš„è®­ç»ƒå›åˆæ•°
    return_list = []  # å­˜å‚¨æ¯ä¸ªå›åˆçš„å›æŠ¥
    moving_avg_rewards = []  # å­˜å‚¨å›æŠ¥çš„æ»‘åŠ¨å¹³å‡
    ma_window = 10  # è®¾ç½®æ»‘åŠ¨å¹³å‡çª—å£å¤§å°

    # è®­ç»ƒè¿‡ç¨‹
    for i in range(num_episodes):
        state, _ = env.reset()  # é‡ç½®ç¯å¢ƒ
        episode_return = 0  # å½“å‰å›åˆçš„æ€»å›æŠ¥
        terminated, truncated = False, False
        while not (terminated or truncated):
            action = agent.take_action(state)  # ç”±æ™ºèƒ½ä½“é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
            next_state, reward, terminated, truncated, _ = env.step(action)  # æ‰§è¡ŒåŠ¨ä½œå¹¶è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±ã€æ˜¯å¦ç»“æŸ
            # clipped_reward = np.clip(reward, -1, 1)  # å¥–åŠ±è£å‰ªè‡³[-1, 1]åŒºé—´
            agent.replay_buffer.add(state, action, reward, next_state, terminated or truncated)  # å°†ç»éªŒåŠ å…¥ç»éªŒæ± 
            agent.update()  # æ›´æ–°æ™ºèƒ½ä½“
            state = next_state  # æ›´æ–°çŠ¶æ€
            episode_return += reward  # ç´¯åŠ å¥–åŠ±

        return_list.append(episode_return)  # å°†å½“å‰å›åˆçš„å›æŠ¥åŠ å…¥å›æŠ¥åˆ—è¡¨
        moving_avg = np.mean(return_list[-ma_window:])  # è®¡ç®—æœ€è¿‘ ma_window ä¸ªå›åˆçš„æ»‘åŠ¨å¹³å‡å›æŠ¥
        moving_avg_rewards.append(moving_avg)

        # å°†è®­ç»ƒå›æŠ¥å’Œæ»‘åŠ¨å¹³å‡å›æŠ¥å†™å…¥ TensorBoard
        agent.writer.add_scalar("Return/train", episode_return, i)
        agent.writer.add_scalar("Return/MovingAverage", moving_avg, i)

        # æ¯10ä¸ªå›åˆæµ‹è¯•ä¸€æ¬¡æ¨¡å‹
        if (i + 1) % 10 == 0:
            avg_return = test_policy(env, agent)  # æµ‹è¯•æ¨¡å‹
            print(f"Episode {i+1}, Return: {episode_return:.2f}, Moving Avg: {moving_avg:.2f}, Test Avg: {avg_return:.2f}")
            agent.writer.add_scalar("Return/test_avg", avg_return, i)  # å°†æµ‹è¯•å›æŠ¥å†™å…¥ TensorBoard

    # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
    # agent.save("dqn_mountaincar.pth")

    # ç»˜åˆ¶å›æŠ¥å›¾å’Œç¨³å®šæ€§å›¾
    plot_metrics(return_list, moving_avg_rewards)
    compute_stability(return_list)

    # print("\nğŸ¬ æ­£åœ¨å½•åˆ¶æ¼”ç¤ºè§†é¢‘ ...")
    # # å½•åˆ¶æ¼”ç¤ºè§†é¢‘
    # record_video(env, agent)
    # print("âœ… è§†é¢‘å·²ä¿å­˜ä¸º mountaincar_demo.mp4")

# è¿è¡Œä¸»å‡½æ•°
if __name__ == '__main__':
    main()